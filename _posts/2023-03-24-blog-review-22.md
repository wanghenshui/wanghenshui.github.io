---
layout: post
title: blog review 第二十二期
categories: [review]
tags: [rocksdb,pg,hive]
---
最近感悟

有些事情自己不接触到是不会相信的，比如系统API不稳定，尤其是write read。有一些歌开源代码我就不说是谁了，系统接口永远默认成功我操

关键是定位问题的盲点也在这里，永远不会去考虑系统的错误，而是查业务自身有没有问题。导致白白浪费时间。在不靠谱的代码上堆叠出来的业务，又咋可能靠谱

<!-- more -->

## rocksdb最后一层的compact问题

最近和sue讨论rockdb值得记录一下:

如果周期性的反复删除重写一组key，这组key最终会落在最后一层，而最后一层的compact调度是比较晚的，最后一层也不参加score计算compact，所以需要主动触发compact来把这种key删除掉

这也是为什么myrocks会有针对delete的计数配置的原因

```
     rocksdb_compaction_sequential_deletes   0
     rocksdb_compaction_sequential_deletes_count_sd  O
     rocksdb_compaction_sequential_deletes_file_size 0
     rocksdb_compaction_sequential_deletes_window    0
```

类似Pika也有一个计数 https://github.com/OpenAtomFoundation/pika/blob/2f71d18ad81fa01e3239e5f8d70dea5b3694ff3d/src/storage/src/redis_hashes.cc#L272

```cpp
  s = db_->Write(default_write_options_, &batch);
  UpdateSpecificKeyStatistics(key.ToString(), statistic);
  return s;
```
原子的。性能太差，我之前改过一个版本直接删了。还是有可取之处的，不过用atomic太扯里。应该thread_local counter


rocksdb的代码，搜

```c++
kBottommostFiles
LevelCompactionBuilder::SetupInitialFiles()
```

就能看到了

## [Erasure Coding versus Tail Latency](https://brooker.co.za/blog/2023/01/06/erasure.html)

backup request是发两次选最快的一次么，总之这种可以降低延迟

发两次，选一次，发M次，选N(`N<M`)次，我操，纠删码的原理，感觉是这种发两次的拓展情况

所以就存在一种用法，一个对象分K个，r个冗余，从K+r里读k个就行了

我操，EC-Cache！重大发现！不好意思，有人想到了https://zhuanlan.zhihu.com/p/24713081 他妈的

当然这种玩法肯定是能降低延迟的，就是得改造

## [Postgres: The Graph Database You Didn&#39;t Know You Had](https://www.dylanpaulus.com/posts/postgres-is-a-graph-database)

```sql
CREATE TABLE nodes (
  id SERIAL PRIMARY KEY,
  data VARCHAR(255)
);
CREATE TABLE edges (
  previous_node INTEGER REFERENCES nodes(id),
  next_node INTEGER REFERENCES nodes(id),
  PRIMARY KEY (previous_node, next_node)
);
```

第一张表是节点，第二张表是第一张表的id和关联

查询依赖

```sql
SELECT id, data
FROM nodes
JOIN edges ON nodes.id = edges.next_node
WHERE edges.previous_node = 1;
```

查id为1 的相邻节点

如果复杂点，查id为1 点相邻节点的相邻节点呢？递归了

用PG特殊功能 WITH RECURSIVE

```sql
WITH RECURSIVE friend_of_friend AS (
  SELECT edges.next_node
  FROM edges
  WHERE edges.previous_node = 1
  UNION
  SELECT edges.next_node
  FROM edges
  JOIN friend_of_friend ON edges.previous_node = friend_of_friend.next_node
)
SELECT nodes.data
FROM nodes
JOIN friend_of_friend ON nodes.id = friend_of_friend.next_node;
```

WITH RECURSIVE语法

```sql
WITH RECURSIVE {name} AS (
  {base case}
  UNION
  {recursive case}
)
```

## [Hive数据导出的几种方式](https://www.cnblogs.com/sheng-sjk/p/13940642.html)

我以为select *会很快，但是不如直接导出HDFS文件。离谱

```sql

# 1
insert overwrite directory '/home/data/' select * from hive_table;

# 2
insert overwrite local directory '/home/data/' select * from hive_table

row format delimited fields terminated by ‘\t’  #字段间用\t分割

stored as textfile;   #导出文件的存储格式为textfile
```

```shell
export table hive_table to '/home/data/';
```

这和上面一样


## [一种基于DAG的MapReduce调度算法](https://www.jsjkx.com/CN/article/openArticlePDF.jsp?id=1319)

节点主动申请任务代替调度器自身调度。

如何根据DAG排序？说了个向上排序，但是评估数据怎么算的？都没说

## 数据本地性感知的 MapReduce负载均衡策略

基于hash分片，一般来说，数据不会不均匀，如果遇到数据倾斜怎么办，这里就是提前采样，判定每个分片数据规模，然后调度的时候拆分一下。

采样一般来说都有，但是基本都是装装样子，没用上


## [Introducing Compute-Compute Separation for Real-Time Analytics](https://rockset.com/blog/introducing-compute-compute-separation/)

<img src="https://images.ctfassets.net/1d31s1aajogl/5PK8lzle4qDpts9MJGv6tV/3a925841f48c4c330c619e8b1002561e/Screen_Shot_2023-02-28_at_5.48.37_PM.png?w=1676&fm=webp" width="80%">

数据导入到ingest节点，ingest生成数据，然后ingest节点复制同步到线上query节点

回顾一下离线导入逻辑，一般都是有个mapreduce系统spark之类，生成线上的数据，sst文件，上传到对象存储，然后db端从对象存储里下载，ingest导入

rockset本身是集成对象存储的，那还拐弯干啥？直接写到对象存储，然后复制一份到SSD，上线，这也就是ingest节点的作用，query节点直接共享SSD，切一个snapshot的功夫

本质上是把MapReduce直接生成的这个步骤放到线上集群的节点来处理了

离线导入的场景，不能满足流式的场景，这种设计，流式的导入也是没问题的

另外，能省掉中间产物上传下载的时间/开销

这种设计query节点也能无限扩

这里有详细的讨论 https://rockset.com/blog/tech-overview-compute-compute-separation/

<img src="https://images.ctfassets.net/1d31s1aajogl/C2k50nHY0BCwi8r8Rdr7E/29f61b941011ef9b9b81c9e588e1ad77/Screen_Shot_2023-04-11_at_5.34.00_PM.png?w=1671&fm=webp" width="80%">

这个图更明确一些