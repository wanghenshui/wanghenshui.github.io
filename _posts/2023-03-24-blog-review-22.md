---
layout: post
title: blog review 第二十二期
categories: [review]
tags: [rocksdb,pg,hive]
---
最近感悟

<!-- more -->

## rocksdb最后一层的compact问题

最近和sue讨论rockdb值得记录一下:

如果周期性的反复删除重写一组key，这组key最终会落在最后一层，而最后一层的compact调度是比较晚的，最后一层也不参加score计算compact，所以需要主动触发compact来把这种key删除掉

这也是为什么myrocks会有针对delete的计数配置的原因

```
     rocksdb_compaction_sequential_deletes   0
     rocksdb_compaction_sequential_deletes_count_sd  O
     rocksdb_compaction_sequential_deletes_file_size 0
     rocksdb_compaction_sequential_deletes_window    0
```

类似Pika也有一个计数 https://github.com/OpenAtomFoundation/pika/blob/2f71d18ad81fa01e3239e5f8d70dea5b3694ff3d/src/storage/src/redis_hashes.cc#L272

```cpp
  s = db_->Write(default_write_options_, &batch);
  UpdateSpecificKeyStatistics(key.ToString(), statistic);
  return s;
```
原子的。性能太差，我之前改过一个版本直接删了。还是有可取之处的，不过用atomic太扯里。应该thread_local counter


rocksdb的代码，搜

```c++
kBottommostFiles
LevelCompactionBuilder::SetupInitialFiles()
```

就能看到了

## [Erasure Coding versus Tail Latency](https://brooker.co.za/blog/2023/01/06/erasure.html)

backup request是发两次选最快的一次么，总之这种可以降低延迟

发两次，选一次，发M次，选N(`N<M`)次，我操，纠删码的原理，感觉是这种发两次的拓展情况

所以就存在一种用法，一个对象分K个，r个冗余，从K+r里读k个就行了

我操，EC-Cache！重大发现！不好意思，有人想到了https://zhuanlan.zhihu.com/p/24713081 他妈的

当然这种玩法肯定是能降低延迟的，就是得改造

## [Postgres: The Graph Database You Didn&#39;t Know You Had](https://www.dylanpaulus.com/posts/postgres-is-a-graph-database)

```sql
CREATE TABLE nodes (
  id SERIAL PRIMARY KEY,
  data VARCHAR(255)
);
CREATE TABLE edges (
  previous_node INTEGER REFERENCES nodes(id),
  next_node INTEGER REFERENCES nodes(id),
  PRIMARY KEY (previous_node, next_node)
);
```

第一张表是节点，第二张表是第一张表的id和关联

查询依赖

```sql
SELECT id, data
FROM nodes
JOIN edges ON nodes.id = edges.next_node
WHERE edges.previous_node = 1;
```

查id为1 的相邻节点

如果复杂点，查id为1 点相邻节点的相邻节点呢？递归了

用PG特殊功能 WITH RECURSIVE

```sql
WITH RECURSIVE friend_of_friend AS (
  SELECT edges.next_node
  FROM edges
  WHERE edges.previous_node = 1
  UNION
  SELECT edges.next_node
  FROM edges
  JOIN friend_of_friend ON edges.previous_node = friend_of_friend.next_node
)
SELECT nodes.data
FROM nodes
JOIN friend_of_friend ON nodes.id = friend_of_friend.next_node;
```

WITH RECURSIVE语法

```sql
WITH RECURSIVE {name} AS (
  {base case}
  UNION
  {recursive case}
)
```

## [Hive数据导出的几种方式](https://www.cnblogs.com/sheng-sjk/p/13940642.html)

我以为select *会很快，但是不如直接导出HDFS文件。离谱

```sql

# 1
insert overwrite directory '/home/data/' select * from hive_table;

# 2
insert overwrite local directory '/home/data/' select * from hive_table

row format delimited fields terminated by ‘\t’  #字段间用\t分割

stored as textfile;   #导出文件的存储格式为textfile
```

```shell
export table hive_table to '/home/data/';
```

这和上面一样
