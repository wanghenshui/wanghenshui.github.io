---
layout: post
title: blog review 第十八期
categories: [review]
tags: [occ,Backoff,dataflow,log,k8s,rocksdb,b-tree,magma,myrocks,index,wisckey]
---

最近感悟:

模块拆分的越来越小，越来越无状态。啥家庭啊，有百人运维团队的么

原来我们没有测试团队叫devops啊

傻逼bazel不支持静态编译 https://github.com/bazelbuild/bazel/issues/14342，吐了。bazel编译要吃掉32G内存，不开swap直接卡死，有病吧，我用cmake怎么没这么多毛病

哎。感觉数据服务的演进还是慢慢把sql的东西捡回来。BS怎么说来着，xx语言像个脱衣舞女吸引了你，但早晚都会把衣服一件一件穿回来，就跟c++差不多

<!-- more -->

## [What is Backoff For?](https://brooker.co.za/blog/2022/08/11/backoff.html)

在乐观事务冲突中，作者将了个点子，`指数退避 Exponential backoff`，降低冲突。也就是推迟一会。推迟多久合适呢，这很影响延迟。按照TCP窗口那种思路。可能

`sleep = min(cap, base * 2 ** attemp)`事件慢慢变长。这个思路

这个的问题在哪里？首先作为事务，你延迟肯定爆炸了。不在乎那点重试事件，但是，这个时间可能完全不需要这个时间。可能重试马上就成功，可能刚好用完这个时间窗

这个指数级别的时间还是太粗糙了

所以加上个随机就解决了`sleep = random_between(0, min(cap, base * 2 ** attemp))`

这个英文来说是引入`Jitter` 字面意思就是抖动一下，让这个时间模糊一下，而不是一直指数级别递增。让客户端们错开

这种`错开`思维有各种各样的展开思路，比如锁分片，比如redis过期时间别相同，差个几秒。

也可以调整边界。不过效果没有这个好。这里有模拟代码 https://github.com/aws-samples/aws-arch-backoff-simulator

这个场景的前提是，都是短时间大量竞争，乐观重试。这种场景。如果强度一直这么大，这种策略会把整体的延迟都拉低一个档次，造成雪崩。这个策略本质是推迟

推迟的本质是当前忙，过一会不忙，如果一直忙，推迟没有任何意义

`The only way to deal with long-term overload is to reduce load, deferring load does not work.`

这种策略做重试反而加重负担。还是用常规的重试策略。这里不是说指数退避重试不行。

一个好的重试策略是啥样的？
经典方案，重试三次。失败就踢掉
复杂方案，重试令牌桶，桶里N个，失败+1成功-1，桶占满就彻底失败，踢掉
统计模式，失败率上升到一定程度就失败，踢掉

这三种方案各有不同适应场景。重试三次，简单，但是很武断，
统计模式也算简单，负载也低。这两种是类似的，问题在于可能下一次就成功了
重拾令牌桶，最悲观场景，和重试三次一样负载高。但成功率更高一些


再结合一下上面的指数退避模式，结合一下。还要考虑业务场景

如果连接固定，那么指数退避 规避推迟一下就挺有效，如果连接非常多，压力一直持续，那么指数退避就不太行

不一定是连接数，也可能是任务数，也可能是线程数。大家都是一个展开思路。不要想的太死

作者给了一组模拟实验，和上面说的差不多

我佩服这哥们的一点就是他总是能把他的想法用代码模拟一下画出图来。我脑子就是一团浆糊，没法明确


## [The future is seamless and collaborative](https://tably.com/blog/seamless-and-collaborative)

介绍物化视图以及维护以及引出dataflow概念以及介绍 Materialize这个公司的产品

以及介绍 双向的dataflow。以及讨论了一波CDRT。你们概念真特么多


## [网络通信中收发数据的正确姿势 ](https://mp.weixin.qq.com/s?__biz=MzU2MTkwMTE4Nw==&mid=2247487609&idx=1&sn=173c9f7095f4aa3f5b341ac7cff89b47&chksm=fc70eb95cb07628321b8f829bceade42fa26a606bc7d4b2ff7e9f4cf4de9908cd0f9a978dc29&scene=21#wechat_redirect)

- 在 select、poll 和 epoll 的 LT 模式下，可以直接设置检测 fd 的可读事件；
- 在 select、poll 和 epoll 的 LT 模式下不要直接设置检测 fd 的可写事件，应该先尝试发送数据，因为 TCP 窗口太小发不出去再设置检测 fd 的可写事件，一旦数据发出去应立即取消对可写事件的检测。
- 在 epoll 的 ET 模式下，需要发送数据时，每次都要设置检测可写事件。

## [Reducing Logging Cost by Two Orders of Magnitude using CLP](https://www.uber.com/en-US/blog/reducing-logging-cost-by-two-orders-of-magnitude-using-clp)

整了个log压缩工具，clp，来处理spark平台每天日增200G的日志数据

原理

<img src="https://blog.uber-cdn.com/cdn-cgi/image/width=2216,quality=80,onerror=redirect,format=auto/wp-content/uploads/2022/09/figure-2.png" alt=""  width="100%">



这玩意是个列存，支持搜索的列存

那为啥不直接用时序数据库呢？效果不是一致的么？

作者只对比了列存压缩和使用gzip zstd压缩率的对比，没有和时序数据库的对比，实际上都是列存带来的优势，用时序不就一劳永逸了？

现成服务我觉得也做好了存储吧，比如fluent-bit 还支持sql

这里也有讨论 https://news.ycombinator.com/item?id=33032996


## [Index Structures, Access Methods, whatever](http://smalldatum.blogspot.com/2018/04/index-structures-access-methods-whatever.html)

当我想到分类的时候，人家已经提前五年分好类在这里等我了


- 基于树Tree组织数据   
  - 基于Page
    - 就地更新Page型， UiP 例子：innodb
    - 复制修改回写型， COW-R COW-S R表示随机刷一个回去，S表示顺序刷回。S没有用的 COW-R典型代表， lmdb wiretiger
  - LSM 例子leveldb
  - index + log 比如rocksdb改良版 wiskey，blobdb，比如tokudb ForestDB （这个有压测，rocksdb吊锤）
- 基于hash组织数据 （牺牲range信息）
  - page bdb支持设置hash模式？ https://github.com/hyc/BerkeleyDB/ 代码我没有研究过
  - LSM 比如hashkv  https://github.com/LSMdb/HashKV SILK 这个我没找着，论文 https://www.cs.cmu.edu/~dga/papers/silt-sosp2011.pdf
  - index + log 比如 bitcask，实际上这个也是 tree，hash tree，杂交品种，也是对数复杂度，另外微软的FASTER 也是这种模式。快。我之前有博客提到过

从经典的Read Update Memory 三角来分析

- b-tree 读不错，牺牲写，最差最差，每次的更新都很小，都发生了回写，对于cache效率，聚集索引好过非聚集索引 
  -  for a clustered index you should cache at least one key/pointer per leaf block but for a non-clustered index you need the entire index in RAM or there will be extra storage reads
  - 空间放大也还好，最大的问题是碎片
- LSM 写不错，牺牲读，空间利用率也可以，最大的问题是额外的浪费CPU（b-tree也得vacuum，也省不了吧 ）以及写放大
- index + log 读写都可以，主要问题在于index本身，需要内存装一大部分

## [Deletes are fast and slow in an LSM](http://smalldatum.blogspot.com/2020/01/deletes-are-fast-and-slow-in-lsm.html) 

single delete API，当且仅当put一次，可以用，不然行为未定义，另外介绍了myrocks的配置 rocksdb_compaction_sequential_deletes 其实就是统计然后compact。一种优化

myrocks使用rocksdb还是比较激进的，一直用最新的

## [Default options in MyRocks ](http://smalldatum.blogspot.com/2018/07/default-options-in-myrocks.html)

基本配置
```txt
    rocksdb_default_cf_options=block_based_table_factory={
        cache_index_and_filter_blocks=1;
        filter_policy=bloomfilter:10:false;
        whole_key_filtering=1};
    level_compaction_dynamic_level_bytes=true;
    optimize_filters_for_hits=true;
    compaction_pri=kMinOverlappingRatio
```

不同层不同压缩策略

```txt
compression_per_level=kNoCompression:kNoCompression:kNoCompression:kCompression:$fast:$fast;bottommost_compression=$slow
```

没啥说的 fast用lz4/snappy，slow用zstd

另外有个compaction triger，如果tombstone过多

```txt
     rocksdb_compaction_sequential_deletes   0
     rocksdb_compaction_sequential_deletes_count_sd  O
     rocksdb_compaction_sequential_deletes_file_size 0
     rocksdb_compaction_sequential_deletes_window    0
```

这个是myrocks层实现的，统计计数，然后NeedCompact

 [代码](https://github.com/facebook/mysql-5.6/commit/fa16b1a5c7f028cf2b89560791cea01babfbbb10#diff-0698cf855f4ad84ec36cf59ae9c3c4f9c8eb3ee56c4692a37a12bb114e94d99c)

对自己的业务有需要的可以自己配置

##  [Magma, a new storage engine for Couchbase ](http://smalldatum.blogspot.com/2022/09/magma-new-storage-engine-for-couchbase.html)

论文 https://www.vldb.org/pvldb/vol15/p3496-lakshman.pdf、

也是index log模式

> A summary of the implementation:
>   writes are persisted to a WAL and then added to the write cache. I assume each document gets a unique seqno at that time.
>    the write cache (RocksDB memtable) buffers KV pairs with 2 skiplists. The paper first describes them as the active and immutable lists. Later it states one skiplist orders by PK and the other by seqno. I believe the later description. When full the documents are appended to the tail of the open log segment and the (key,seqno, doc size) tuples are flushed to the LSM index.
>   an LSM tree index stores (key, seqno, doc size) tuples to map a document PK to a seqno
>   the Log Structured Object Store is composed of log segments. Per-segment metadata includes the min and max seqno in that segment and the seqno values do not overlap between segments.
>   there is a document cache and index block cache. The index block cache caches blocks from the LSM index and from the per-segment B-tree indexes. 



这里也有介绍 https://zhuanlan.zhihu.com/p/572612966
> 相比于经典的 WiscKey，这篇论文最大的贡献在于 GC 的设计，其不需要像 WiscKey 在 GC 时扫描整个 value log 并进行 point get 查询，而是首先对 key sst 进行 compact 产生 deleted key set，再根据 deleted key set 中记录的 value sequence id 计算得出每段 value log 对应的垃圾数据的比例，只需要对垃圾比例较高的 value log 段与邻接段进行 merge 即可。这样既可以避免 WiscKey GC 时的大量 point get，也可以根据垃圾比例对 value log 进行分段 compact，降低写入放大。

这个问题在于空间放大吧，空洞率不满足条件的占用无法释放。但是GC不扫描log文件还是挺有新意的。fasterkv 的compact就要扫描log文件，这个读放大是非常恐怖的。如果能得知每段文件哪些key被删了 ，效果确实会好很多

不过fasterkv还有个问题，就是log文件本身也有链表信息，这个信息很难受，不能删掉

## [Hyping the hyper clock cache in RocksDB ](http://smalldatum.blogspot.com/2022/10/hyping-hyper-clock-cache-in-rocksdb.html)

rocksdb 7.7.3 hyper clock cache比LRU cache有巨大性能提升

## [Small servers for database performance tests ](http://smalldatum.blogspot.com/2017/05/small-servers-for-database-performance.html)

老哥的NUC配置，我的笔记本也打算这么搞一下

mount配置 
```fstab
UUID=...  /data  xfs  noatime,nodiratime,discard,noauto  0 1
```
这个time啥的我没配置

调试工具，ubuntu默认关闭

```bash
echo -1 > /proc/sys/kernel/perf_event_paranoid
echo 0 > /proc/sys/kernel/yama/ptrace_scope
sudo sh -c " echo 0 > /proc/sys/kernel/kptr_restrict"
```

关闭大页

```bash
echo $1 > /sys/kernel/mm/transparent_hugepage/defrag
echo $1 > /sys/kernel/mm/transparent_hugepage/enabled
cat /sys/kernel/mm/transparent_hugepage/defrag
cat /sys/kernel/mm/transparent_hugepage/enabled
```

或者放在grub里

```bash
GRUB_CMDLINE_LINUX_DEFAULT="transparent_hugepage=never"
update-grub
```

网络设置就算了

查看ssd

```bash
sudo nvme smart-log /dev/nvme0
smartctl -A /dev/sda
```
## [Building a high-performance database buffer pool in Zig using io_uring's new fixed-buffer mode](https://gavinray97.github.io/blog/io-uring-fixed-bufferpool-zig)

大部分linux内核都死在5.x了。想用也用不到

## [Simple, Fast, and Scalable Reverse Image Search Using Perceptual Hashes and DynamoDB](https://canvatechblog.com/simple-fast-and-scalable-reverse-image-search-using-perceptual-hashes-and-dynamodb-df3007d19934)

一个图用相似hash算法生成几段hash 比较图片就比较hash的汉明距离，因为图片相似的汉明距离肯定低，通过这两种来实现相似图片查找。拿dynamodb当kv用。


## k8s分析流程图


<img src="https://learnk8s.io/a/a-visual-guide-on-troubleshooting-kubernetes-deployments/troubleshooting-kubernetes.zh_cn.v4.png" alt=""  width="100%">


## [Umbra: A Disk-Based System with In-Memory Performance (Thomas Neumann)](https://www.youtube.com/watch?v=pS2_AJNIxzU&list=PLSE8ODhjZXjZKp-oX_75aBnznulk7nubu)

论文在这里 https://www.cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf
他这个ppt我没有。讲的很多是buffer pool的设计，跟 这个博客说的差不多 https://nan01ab.github.io/2020/12/Umbra.html

我直接把结论贴过来吧。没开源，看不出什么东西来

有一些设计学的 https://github.com/leanstore/leanstore


## Database Implementation For Modern Hardware - Thomas Neumann

这哥们是hyper前开发，创业了，整了个umbra。这个是他讲的课。我简单根据ppt总结一下

有点长，当最后一个了

### disk

Techniques to speed up disk access:
• do not move the head for every single tuple
• instead, load larger chunks
• typical granularity: one page
• page size varies. traditionally 4KB, nowadays often 16K and more
• page size is a trade-off

The page structure is very prominent within the DBMS
• granularity of I/O
• granularity of buffering/memory management
• granularity of recovery
Page is still too small to hide random I/O though
• sequential page access is important
• DBMSs use read-ahead techniques
• asynchronous write-back

### buffer

Some pages are accessed very frequently
• reduce I/O by buffering/caching
• buffer manager keeps active pages in memory
• limited size, discards/write back when needed
• coupled with recovery, in particular logging
Basic interface:
1. FIX(pageNo,shared)
2. UNFIX(pageNo,dirty)
Pages can only be accessed (or modified) when they are fixed.

#### Buffer Frame
Maintains the state of a certain page within the buffer
- pageNo the page number
- latch a read/writer lock to protect the page(note: must not block unrelated pages!)
- LSN: LSN of the last change, for recovery (buffer manager must force the log before writing)
- state clean/dirty/newly created etc.
- data the actual data contained on the page

(will usually contain extra information for buffer replacement)

Usually kept in a hash table

#### Buffer Replacement
FIFO, LRU, 2Q, LFU

#### Buffer Replacement second chance
LRU is nice, but the LRU list is a hot spot.
Idea: use a simpler mechanism to simulate LRU
• one bit per page
• bit is set when page is unfixed
• when replacing, replace pages with unset bit
• set bits are cleared during the process
• strategy is called “second chance” or “clock

#### Buffer Replacement 2Q
Maintain not one queue but two
• many pages are referenced only once
• some pages are hot and reference frequently
• maintain a separate list for those
1. maintain all pages in FIFO queue
2. when a page is references again that is currently in FIFO, move it into
an LRU queue
3. prefer evicting from FIFO
Hot pages are in LRU, read-once pages in FIFO. Good strategy for common
DBMS operations.

#### Buffer Replacement  Hints
Application knowledge can help buffer replacement
• 2Q tries to recognize read-once pages
• these occur when scanning over data
• but the DBMS knows this anyway!
• it could therefore give hints when unfixing
• e.g., will-need, or will-not-need (changes placement in queue)


### Segments
While page granularity is fine for I/O, it is somewhat unwieldy
• most structures within a DBMS span multiple pages
• relations, indexes, free space management, etc.
• convenient to treat these as one entity
• all DBMS pages are partitioned into sets of pages
Such a set of pages is called a segment.
Conceptually similar to a file or virtual memory.


#### Shadow Paging 

uses a page table, dirty pages are stored in a shadow copy.

Advantages:
• the clean data is always available on disk
• greatly simplified recovery
• can be used for transaction isolation, too
Disadvantages:
• complicates the page access logic
• destroys data locality
Nowadays rarely used in disk-based systems


#### Delta Files 

Similar idea to shadow paging:
• on change pages are copied to a separate file
• a copied page can be changed in-place
• on commit discard the file, on abort copy back
Can be implemented in two flavors:
• store a clean copy in the delta
• store the dirty data in the delta
Both have pros and cons.


Delta files have some advantages over shadow paging:
• preserve data locality
• no mixture of clean and dirty pages
Disadvantages:
• cause more I/O
• abort (or commit) becomes expensive
• keeping track of delta pages is non-trivial
Still, often preferable over shadow paging.


#### 数据结构
- 空间管理 
    - Free Space Inventory
    - slot page管理
        - 多个事务操作同一个page引发冲突 -> TID直接放到slot里
- 数据落盘，数据怎么存？TLV？硬编码？padding？
    - 非常规数据
        - 如何表达NULL？
    - 压缩问题
        - 单条压缩，访问性能差，有依赖问题，dict 压缩可行
    - 长数据问题，改成指针指向文件
        - BLOB类型的优化，用户可能会乱用，所以你得优化BLOB短的场景
- 索引落地
    - b tree ->  b+tree 
        - 查找二分优势


slot page定义

Header:
- LSN for recovery
- slotCount number of used slots
- firstFreeSlot to speed up locating free slots
- dataStart lower end of the data
- freeSpace space that would be available after compactification

Note: a slotted page can contain hundreds of entries!
Requires some care to get good performance.

特殊slot
free slot offset = 0 len=0 
空slot offset > 0 len=0

record格式
int int len str int len str， TLV

---

看到这里或许你有建议或者疑问或者指出我的错误，请留言评论或者邮件mailto:wanghenshui@qq.com, 多谢!  你的评论非常重要！

<details>
<summary>觉得写的不错可以点开扫码赞助几毛</summary>
<img src="https://wanghenshui.github.io/assets/wepay.png" alt="微信转账">
</details>
