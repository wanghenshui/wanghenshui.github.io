---
layout: post
title: blog review 第二十四期
categories: [review]
tags: []
---

最近感悟

划水太严重要被开了

<!-- more -->

https://decentralizedthoughts.github.io/2020-12-12-raft-liveness-full-omission/


https://quickwit.io/blog/compressed-indexable-bitset


https://justinjaffray.com/joins-13-ways/

https://dotat.at/prog/qp/README.html

https://mp.weixin.qq.com/s/sbY0g65nETTbqPjV5MC80w?vid=1688851418274222


## HLC

Physical Time Logical Clock PT组成作为逻辑时钟的高位， LC作为低位。

比如我们可以采用一个64bit的整数来存储一个HLC timestamp，那么这里简单定义，高32bit存储PT，低32bit存储LC。

每一个节点除了有一个HLC Clock，还有一个物理时钟（physical clock），这个物理时钟可以假设是任何的物理时钟，比如ntp协议，或者google的true time协议，这里对于物理时钟没有任何要求，物理时钟可以跳变，也可以回退，允许存在误差。


一个HLC Timestamp比较大小，也是先比较 physical time，如果physical time相等，再比较logical time

## [深入理解 Linux 内核--jemalloc 引起的 TLB shootdown 及优化](https://juejin.cn/post/6844904084957315086)

如果有IPI，需要关注一下是不是jemalloc帮倒忙了

```bash
watch -d -n 1 "cat /proc/interrupts | grep TLB" #观察数量
perf top #smp_call_function_many
ls /proc/*/maps | xargs grep jemalloc #观察jemalloc
strace -f -p 1510 2>&1 | grep madvise #观察有没有 MADV_DONTNEED

```

解决办法 

```bash
MALLOC_CONF=dirty_decay_ms:-1,muzzy_decay_ms:-1
```

## [why this is bad to use XFS file-system without d_type support , regarding to docker](https://superuser.com/questions/1576757/why-this-is-bad-to-use-xfs-file-system-without-d-type-support-regarding-to-doc)

```bash
mkfs.xfs -f -n ftype=1 /dev/sda4
```

## [I/O Acceleration with Pattern Detection](https://www.cs.cmu.edu/~garth/papers/he-hpdc13.pdf)

## 一些设计的快速估算

访问密度

T = (QPS + TPS*N) / TotolStorageG

数据规模

一亿条x4KB = 400G数据 一亿八个零，去掉九个0是G，0.1 x 4000 = 400G

云上机器SSD是 PM9A3，查了一下资料，国产固态新品基本上都比这个盘效果好。比如致态Tiplus 7100。我得买点这个


信息来自 techpowerup 以及这个 https://www.zhihu.com/tardis/zm/art/580123162?source_id=1003


| 磁盘型号 | 顺序读带宽 | 顺序写带宽 |随机读IOPS |随机写IOPS |
| --- | --- | --- | --- | --- |
| PM9A3 |6800 MB/s  | 4000MB/s|1000k IOPS|180k IOPS|
| 致态TiPlus7000 |  7000 MB/s| 6100 MB/s |970k IOPS |770k  IOPS|

有群友反应消费级磁盘会降速。需要重新测试一下，我怀疑是随机读写导致的碎片降速

一个盘2T， 每个分片10G，保留25%容量用来compact，也就还剩150个db，假设每个都是一个副本的一个分片（避免多个分片落在同一个盘上一崩崩俩）也就是150个副本

元数据信息，假设一个副本的所有分片信息，都存，一条2K，2K * 150 一个节点的量。节点和磁盘数字正相关，元数据节点要考虑数据路由管理的限制

什么时候会修改元数据

- 主动迁移，腾空机器，均衡机器

- 被动迁移，坏盘

- 副本元数据变动
    - 更改kv引擎的配置参数，更改表结构schema，复制组角色变化（这个可能频繁）

copyset设定

> copyset原理，划分了不同的隔离域，例如标准集群中，选取3个满足可用区标准的27台机器为一个管理单元，
> 并且保证同一个AZ内的9台机器属于3个不同的机架，并根据机架关系，27台机器划分出9个升级域
> 同一个Partition的不同副本一定分散在不同的升级域内，
> 以此来保证在坏盘、死机、机架掉电以及机房故障等场景下，依然可以保证服务高可靠和高可用


如何全局中心来调度？

- 还是主动迁移被动迁移，raft副本坏了可以快速分配log复制模块，只保证日志正确就行，witness，可以不成为主，只要保证复制就行，节点恢复正常自动退出

- 采集信息，根据访问密度分出几档，然后调度迁移/切主

k8s信息 https://www.huweihuang.com/kubernetes-notes/code-analysis/kube-controller-manager/sharedIndexInformer.html


## [Measuring Memory Subsystem Performance](https://johnnysswlab.com/measuring-memory-subsystem-performance/)

介绍几种测量cache使用的工具

代码在这 https://github.com/ibogosavljevic/johnysswlab/blob/master/2023-07-metrics/multithreading.cpp#L60

工具是这个 https://github.com/RRZE-HPC/likwid

## 重新考虑编码问题

常规的kv编码就是TLV格式

总长度 + key 长度 + key + value1长度 + value1 + value2长度 + value2


对于kv引擎来说，key单独抠出来，value加上一些属性，比如version/ttl/子keysize 等等，也可以不需要size

多个value怎么存？

- 拆成hash模式，key链，穿起来，pika的编码层就是这样的
    - metakey fieldkey -> value 类似TLV 是length value 模式  -> 放在后面也可以？更方便comparetor？不然compare得跳过开头的长度, 其实理论上差不多
    - 兼容问题，是否需要编进去一个encode_type
    - 是否需要分片hash？
        - 是否需要slot级别的scan？这种scan其实对全局scan影响很大。我觉得不是很有必要，如果需要，编码里还得加个hashkey(int32)
        - 更细粒度的分裂/合并，不过你理论上已经有了这个信息(路由)了，编到key里有点多此一举，除非你需要特别精细的力度，抓到热key然后把这个key对应的hashkey范围撕出来
        - 对于range路由，完全没用
- 编成巨长的一行，protobuf/fbs